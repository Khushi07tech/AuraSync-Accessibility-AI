# ‚ú®AuraSync-Accessibility-AI
Bridging the Context Gap with <strong> Multimodal AI Reasoning </strong>
AuraSync is a human-centric accessibility platform that transforms visual media into immersive experiences for the Visually Impaired and Hearing Impaired. By leveraging the multimodal reasoning of Gemini 3 Flash, AuraSync doesn't just describe what is happening‚Äîit explains the context and emotion behind the screen.

# üìΩÔ∏è The Problem:
Current accessibility tools often fail at <em> "Translating the Experience" </em>:

<h3><u>For the Blind:</u></h3>Traditional descriptions are often robotic.

<h3><u>For the Deaf:</u></h3> Sound captions are often literal (e.g., "[Noise]") or rely on a grasp of spoken language that may be a second language for those born profoundly deaf.

# üöÄ The AuraSync Solution
AuraSync introduces a dual-stream processing engine that prioritizes Visual-Spatial Narrative:

<h3><u>Cinematic Narration:</h3></u> For the visually impaired, AuraSync generates vivid, emotional descriptions of micro-expressions and lighting. A custom Regex Narrative Filter ensures the user hears a cohesive story.

<h3><u>Environmental Subtitles:</h3></u> For the hearing impaired, we pivot from "sounds" to "sources." By describing the physical impact of a sound (e.g., "[The vibration shakes the glass]"), we provide a sensory bridge that is more intuitive than abstract text.

# üß† Technical Architecture
1. <h3><u>Dynamic Reasoning Depth</h3></u> <br>
AuraSync utilizes Gemini 3 Flash's native Thinking Mode. The "Thinking Depth" slider allows the model to "show its work" and reason through complex scenes before providing a final output, ensuring high accuracy for intricate visual actions.


2. <h3><u>Universal Synchronized Narration</h3></u> <br>
AuraSync utilizes a Unified Temporal Reference System that anchors every descriptive narrative to a precise frame-index. Rather than isolating the description from the timeline, our architecture integrates audible timestamps to serve as "spatial anchors" for the user. This design choice ensures absolute structural consistency, allowing users‚Äîparticularly those using screen readers or assistive cross-referencing tools‚Äîto maintain a frame-accurate understanding of exactly when a visual or environmental event occurs within the video‚Äôs progression.

# üõ†Ô∏è Tech Stack
<strong><u>AI Model</u></strong>: Google Gemini 3 Flash<br>
<strong><u>Framework:</strong></u> Streamlit<br>
<strong><u>Audio Engine:</strong></u> gTTS (Google Text-to-Speech)<br>
<strong><u>Logic:</strong></u> Python 3.10+ with Regular Expression (Regex) Filtering

# üåç Inclusive Design & Future Goals
We recognize that for the profoundly deaf, sign language is often their primary language.

<h3><u>Current Innovation:</h3></u> Using concrete, active verbs and physical descriptions to create "Visual Grammar" in text.

<h3><u>The Roadmap:</h3></u> Future versions aim to integrate Sign Language Translation (SLT) avatars to convert AI reasoning directly into spatial sign language.

# üì¶ Setup & Installation
Clone the Repository <br>
Install Requirements <br>
Set your API Key: Add your Gemini API key to engine.py. <br>
Run the App 

# üë• Contributor
Khushbakht Irfan ‚Äî Design & AI Development
